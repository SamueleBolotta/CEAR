{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 Exercise CIFAR10 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_l9pfkj42M5",
    "outputId": "9a583018-adbd-4c47-ac1c-62ffa08d935c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "Torchvision version: 0.25.0+cu128\n",
      "Device: cuda\n",
      "CUDA disponibile: True\n"
     ]
    }
   ],
   "source": [
    "# Import librerie\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Scikit-learn per preprocessing e metriche\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Impostazioni\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
    "\n",
    "import os, urllib.request\n",
    "\n",
    "# GitHub Release URL for pretrained weights\n",
    "WEIGHTS_BASE_URL = os.environ.get('WEIGHTS_URL', 'https://github.com/SamueleBolotta/CEAR/releases/download/v1.0/')\n",
    "WEIGHTS_DIR = '../pretrained_weights'\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "def load_or_train(model, train_fn, weights_filename, device='cpu'):\n",
    "    \"\"\"Load pretrained weights if available, otherwise train and save.\n",
    "    Also saves/loads training history as JSON alongside weights.\"\"\"\n",
    "    weights_path = os.path.join(WEIGHTS_DIR, weights_filename)\n",
    "    history_path = weights_path.replace('.pt', '_history.json')\n",
    "\n",
    "    def _load_history():\n",
    "        if os.path.exists(history_path):\n",
    "            import json as _json\n",
    "            with open(history_path, 'r') as f:\n",
    "                return _json.load(f)\n",
    "        return None\n",
    "\n",
    "    if os.path.exists(weights_path):\n",
    "        model.load_state_dict(torch.load(weights_path, map_location=device, weights_only=True))\n",
    "        print(f\"Loaded pretrained weights from {weights_path}\")\n",
    "        return _load_history()\n",
    "    elif WEIGHTS_BASE_URL:\n",
    "        try:\n",
    "            url = WEIGHTS_BASE_URL + weights_filename\n",
    "            urllib.request.urlretrieve(url, weights_path)\n",
    "            # Also try downloading history\n",
    "            try:\n",
    "                urllib.request.urlretrieve(\n",
    "                    WEIGHTS_BASE_URL + weights_filename.replace('.pt', '_history.json'), history_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "            model.load_state_dict(torch.load(weights_path, map_location=device, weights_only=True))\n",
    "            print(f\"Downloaded and loaded weights from {url}\")\n",
    "            return _load_history()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not download weights: {e}. Training from scratch...\")\n",
    "\n",
    "    history = train_fn()\n",
    "    torch.save(model.state_dict(), weights_path)\n",
    "    print(f\"Saved weights to {weights_path}\")\n",
    "    if history is not None:\n",
    "        import json as _json\n",
    "        with open(history_path, 'w') as f:\n",
    "            _json.dump(history, f)\n",
    "        print(f\"Saved training history to {history_path}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Esercizio 3"
   ],
   "metadata": {
    "id": "2WOExwGNVdAq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# ESERCIZIO 3: Tecniche di Regolarizzazione per Ridurre Overfitting\n",
    "# ============================================================================\n",
    "# Task: Confrontare Dropout, L2 Regularization e Batch Normalization\n",
    "# Dataset: CIFAR-10 subset con 2 classi (10000 campioni)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Caricamento subset CIFAR-10: solo classi 0 (airplane) e 1 (automobile)\n",
    "np.random.seed(789)\n",
    "torch.manual_seed(789)\n",
    "cifar_train = torchvision.datasets.CIFAR10(root='../data', train=True, download=True)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='../data', train=False, download=True)\n",
    "\n",
    "X_cifar = cifar_train.data  # (50000, 32, 32, 3)\n",
    "y_cifar = np.array(cifar_train.targets)\n",
    "X_test_cifar = cifar_test.data\n",
    "y_test_cifar = np.array(cifar_test.targets)\n",
    "\n",
    "raise NotImplementedError()\n",
    "\n",
    "X_train_cifar = X_cifar[mask_train]\n",
    "y_train_cifar = y_cifar[mask_train]\n",
    "X_test_cifar_sub = X_test_cifar[mask_test]\n",
    "y_test_cifar_sub = y_test_cifar[mask_test]\n",
    "\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"Dataset CIFAR-10 Binario (Airplane vs Automobile)\")\n",
    "print(f\"Train: {X_train_cifar.shape}, Test: {X_test_cifar_sub.shape}\")\n",
    "print(f\"Distribuzione train: Airplane={np.sum(y_train_cifar==0)}, Auto={np.sum(y_train_cifar==1)}\")\n",
    "\n",
    "# Helper function for training binary CIFAR models with early stopping\n",
    "def train_binary_cifar(model, X_train_np, y_train_np, epochs=20, batch_size=128, val_split=0.2, patience=3, weight_decay=0.0):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "\n",
    "    X_t = torch.FloatTensor(X_train_np).to(device)\n",
    "    y_t = torch.FloatTensor(y_train_np).to(device)\n",
    "\n",
    "    n_val = int(len(X_t) * val_split)\n",
    "    idx = torch.randperm(len(X_t))\n",
    "    X_tr, y_tr = X_t[idx[n_val:]], y_t[idx[n_val:]]\n",
    "    X_vl, y_vl = X_t[idx[:n_val]], y_t[idx[:n_val]]\n",
    "\n",
    "    train_ds = TensorDataset(X_tr, y_tr)\n",
    "    loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for bx, by in loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(bx).squeeze()\n",
    "            loss = criterion(out, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * bx.size(0)\n",
    "            predicted = (torch.sigmoid(out) > 0.5).float()\n",
    "            total += by.size(0)\n",
    "            correct += (predicted == by).sum().item()\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(X_vl).squeeze()\n",
    "            val_loss_val = criterion(val_out, y_vl).item()\n",
    "            val_pred = (torch.sigmoid(val_out) > 0.5).float()\n",
    "            val_acc = (val_pred == y_vl).sum().item() / len(y_vl)\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss_val)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        if val_loss_val < best_val_loss:\n",
    "            best_val_loss = val_loss_val\n",
    "            patience_counter = 0\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                model.load_state_dict(best_state)\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "def evaluate_binary_cifar(model, X_np, y_np):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.FloatTensor(X_np).to(device)\n",
    "        out = model(X_t).squeeze()\n",
    "        raise NotImplementedError()\n",
    "        acc = (pred == y_np).mean()\n",
    "    return acc\n",
    "\n",
    "# Step 1: Modello baseline (senza regolarizzazione)\n",
    "model_baseline_c = nn.Sequential(\n",
    "    nn.Linear(3072, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 128), nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)\n",
    "\n",
    "print(\"Modello Baseline:\")\n",
    "print(model_baseline_c)\n",
    "print(f'Parametri: {sum(p.numel() for p in model_baseline_c.parameters()):,}')\n",
    "\n",
    "# Step 2: Modello con Dropout\n",
    "model_dropout_c = nn.Sequential(\n",
    "    nn.Linear(3072, 256), nn.ReLU(), nn.Dropout(0.4),\n",
    "    nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.4),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)\n",
    "\n",
    "# Step 3: Modello con L2 Regularization (weight_decay nell'optimizer)\n",
    "model_l2_c = nn.Sequential(\n",
    "    nn.Linear(3072, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 128), nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)\n",
    "\n",
    "# Step 4: Modello con Batch Normalization\n",
    "model_bn_c = nn.Sequential(\n",
    "    nn.Linear(3072, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "    nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)\n",
    "\n",
    "# Training tutti i modelli (with pretrained weight support)\n",
    "models_dict_c = {\n",
    "    'Baseline': (model_baseline_c, 0.0, 'nb04_ex3_baseline.pt'),\n",
    "    'Dropout': (model_dropout_c, 0.0, 'nb04_ex3_dropout.pt'),\n",
    "    'L2 Regularization': (model_l2_c, 0.01, 'nb04_ex3_l2.pt'),\n",
    "    'Batch Normalization': (model_bn_c, 0.0, 'nb04_ex3_batchnorm.pt')\n",
    "}\n",
    "\n",
    "histories_c = {}\n",
    "results_c = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MODELLI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, (model_c, wd, wf) in models_dict_c.items():\n",
    "    print(f'\\nTraining {name}...')\n",
    "    history = load_or_train(\n",
    "        model_c,\n",
    "        lambda m=model_c, w=wd: train_binary_cifar(m, X_train_cifar, y_train_cifar, epochs=20, weight_decay=w),\n",
    "        wf,\n",
    "        device=device,\n",
    "    )\n",
    "    histories_c[name] = history\n",
    "\n",
    "    test_acc = evaluate_binary_cifar(model_c, X_test_cifar_sub, y_test_cifar_sub)\n",
    "\n",
    "    if history is not None:\n",
    "        train_acc = max(history['accuracy'])\n",
    "        val_acc = max(history['val_accuracy'])\n",
    "        overfitting_gap = train_acc - val_acc\n",
    "        n_epochs = len(history['loss'])\n",
    "    else:\n",
    "        train_acc = evaluate_binary_cifar(model_c, X_train_cifar, y_train_cifar)\n",
    "        val_acc = None  # no validation data available with pretrained weights\n",
    "        overfitting_gap = None\n",
    "        n_epochs = 'N/A'\n",
    "\n",
    "    results_c.append({\n",
    "        'modello': name,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'epochs': n_epochs\n",
    "    })\n",
    "\n",
    "    print(f'{name}: Test Acc={test_acc:.4f}, Overfitting Gap={f\"{overfitting_gap:.4f}\" if overfitting_gap is not None else \"N/A (pretrained)\"}')\n",
    "\n",
    "results_df_c = pd.DataFrame(results_c).sort_values('test_acc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFRONTO TECNICHE REGOLARIZZAZIONE\")\n",
    "print(\"=\"*70)\n",
    "print(results_df_c.to_string(index=False))\n",
    "\n",
    "# Step 5: Visualizzazione learning curves\n",
    "if any(h is not None for h in histories_c.values()):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (name, history) in enumerate(histories_c.items()):\n",
    "        ax = axes[idx]\n",
    "        if history is not None:\n",
    "            ax.plot(history['accuracy'], label='Train')\n",
    "            ax.plot(history['val_accuracy'], label='Validation')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Pretrained\\n(no curves)',\n",
    "                    ha='center', va='center',\n",
    "                    transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title(f'{name}')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Using pretrained weights - learning curves not available\")\n",
    "\n",
    "# Confronto overfitting gap\n",
    "has_gaps = any(g is not None for g in results_df_c['overfitting_gap'].values)\n",
    "if has_gaps:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plot_df = results_df_c[results_df_c['overfitting_gap'].notna()]\n",
    "    models_names = plot_df['modello'].values\n",
    "    gaps = plot_df['overfitting_gap'].values.astype(float)\n",
    "    colors = ['red' if gap > 0.05 else 'green' for gap in gaps]\n",
    "\n",
    "    ax.barh(models_names, gaps, color=colors, alpha=0.7)\n",
    "    ax.axvline(x=0.05, color='orange', linestyle='--', label='Soglia Overfitting')\n",
    "    ax.set_xlabel('Overfitting Gap (Train Acc - Val Acc)')\n",
    "    ax.set_title('Confronto Overfitting: Tecniche di Regolarizzazione')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Overfitting gap non disponibile con pesi pretrained (servono dati di validazione)\")\n",
    "\n",
    "best_model_name_c = results_df_c.iloc[0]['modello']\n",
    "print(f'\\nMigliore tecnica: {best_model_name_c}')\n",
    "print(f\"Test Accuracy: {results_df_c.iloc[0]['test_acc']:.4f}\")\n",
    "og = results_df_c.iloc[0]['overfitting_gap']\n",
    "print(f\"Overfitting Gap: {f'{og:.4f}' if og is not None else 'N/A (pretrained)'}\")\n",
    "\n",
    "print(\"\\nEsercizio 3 completato!\")\n",
    "\n",
    "# Ripristino seed globali\n",
    "np.random.seed(42)\n",
    "_ = torch.manual_seed(42)"
   ],
   "metadata": {
    "id": "PjbZvAxWVeXP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "15dee301-866f-4c54-93f9-b01cdbad28d0"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 170M/170M [00:04<00:00, 40.8MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset CIFAR-10 Binario (Airplane vs Automobile)\n",
      "Train: (10000, 3072), Test: (2000, 3072)\n",
      "Distribuzione train: Airplane=5000, Auto=5000\n",
      "Modello Baseline:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Parametri: 819,713\n",
      "\n",
      "======================================================================\n",
      "TRAINING MODELLI\n",
      "======================================================================\n",
      "\n",
      "Training Baseline...\n",
      "Downloaded and loaded weights from https://github.com/SamueleBolotta/CEAR/releases/download/v1.0/nb04_ex3_baseline.pt\n",
      "Baseline: Test Acc=0.8635, Overfitting Gap=N/A (pretrained)\n",
      "\n",
      "Training Dropout...\n",
      "Downloaded and loaded weights from https://github.com/SamueleBolotta/CEAR/releases/download/v1.0/nb04_ex3_dropout.pt\n",
      "Dropout: Test Acc=0.8575, Overfitting Gap=N/A (pretrained)\n",
      "\n",
      "Training L2 Regularization...\n",
      "Downloaded and loaded weights from https://github.com/SamueleBolotta/CEAR/releases/download/v1.0/nb04_ex3_l2.pt\n",
      "L2 Regularization: Test Acc=0.8515, Overfitting Gap=N/A (pretrained)\n",
      "\n",
      "Training Batch Normalization...\n",
      "Downloaded and loaded weights from https://github.com/SamueleBolotta/CEAR/releases/download/v1.0/nb04_ex3_batchnorm.pt\n",
      "Batch Normalization: Test Acc=0.8845, Overfitting Gap=N/A (pretrained)\n",
      "\n",
      "======================================================================\n",
      "CONFRONTO TECNICHE REGOLARIZZAZIONE\n",
      "======================================================================\n",
      "            modello  train_acc val_acc  test_acc overfitting_gap epochs\n",
      "Batch Normalization     0.8991    None    0.8845            None    N/A\n",
      "           Baseline     0.8780    None    0.8635            None    N/A\n",
      "            Dropout     0.8613    None    0.8575            None    N/A\n",
      "  L2 Regularization     0.8512    None    0.8515            None    N/A\n",
      "Using pretrained weights - learning curves not available\n",
      "Overfitting gap non disponibile con pesi pretrained (servono dati di validazione)\n",
      "\n",
      "Migliore tecnica: Batch Normalization\n",
      "Test Accuracy: 0.8845\n",
      "Overfitting Gap: N/A (pretrained)\n",
      "\n",
      "Esercizio 3 completato!\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}